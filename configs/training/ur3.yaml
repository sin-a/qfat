defaults:
  - /entrypoints/TrainingEntrypointCfg@_here_
  - /wandb/WandbCfg@wandb_cfg
  - /datasets/SlicedTrajectoryDatasetCfg@slicer_cfg
  - /models/QFATCfg@model_cfg
  - /optimizers/OptimizerCfg@model_cfg.optimizer_cfg
  - /models/IdentityEncoderCfg@model_cfg.encoder_cfg
  - /models/DecoderBlockCfg@model_cfg.decoder_block_cfg
  - /models/MultiheadAttentionCfg@model_cfg.decoder_block_cfg.mha_cfg
  - /datasets/UR3TrajectoryDatasetCfg@training_dataset_cfg
  - /dataloaders/DataLoaderCfg@train_dataloader_cfg
  - _self_
seed: 42
wandb_cfg:
  name: ${model_cfg.mixture_size}mix
  group: ur3 
  project: qfat
  id: # set id here to resume training from a checkpoint 
  notes:
  mode: online
training_dataset_cfg:
  stats_path: data/ur3/data_stats.json
train_dataloader_cfg:
  shuffle: true
  batch_size: 2048 
  num_workers: 0
  persistent_workers: false
val_dataloader_cfg:
  shuffle: false 
  batch_size: 1024
  num_workers: 0
  persistent_workers: false
model_cfg:
  full_covariance: false
  n_layer: 12
  embd_dropout: 0.1
  history_mask_prob: 0
  context_len: 10 
  input_dim: 6
  out_dim: 2 # action_dim*(action_horizon + 1)
  mixture_size: 4
  variance_tol: 1e-5
  optimizer_cfg:
    n_epochs: 400
    use_cosine_schedule: true
    learning_rate: 0.001
    warmup_epochs: 30 # ignored if use_cosine_schedule is false
    min_lr: 0.000001 # ignored if use_cosine_schedule is false
  decoder_block_cfg:
    residual_dropout: 0.1
    mha_cfg:
      embed_dim: 128
      num_heads: 8
      dropout: 0.1
  lambda_mixtures: 0
slicer_cfg:
  window: 10 # ${model_cfg.context_len} 
  action_horizon: 0
runtime_transforms:
  - _target_: qfat.runtime_transforms.runtime_transforms.CosineGaussianPerturbation 
    state_initial_noise: 0
    state_final_noise: 0
    state_input_index:
    action_initial_noise: 0
    action_final_noise: 0
    action_input_index: null
    log_noise: false
    max_epoch: ${model_cfg.optimizer_cfg.n_epochs} 
    fixed_noise: false
callbacks:
  epoch_end:
    - _target_: qfat.callbacks.train_callbacks.RolloutCallback
      env_cfg:
        _target_: qfat.environments.ur3.ur3_wrapper.UR3Wrapper
      sampler_cfg:
        _target_: qfat.samplers.sampler.AutoRegressiveSampler
        context_len: # leave blank to use the model's context length
        temperature: 1
        sample_fn: gmm
        horizon: ${slicer_cfg.action_horizon}
      reward_reduction: sum 
      frequency: ${n_save_model}
      n_rollouts: 50
      max_steps: 1000
      log_entropy: true
      min_sequence_length: 2
      max_sequence_length: 2
      skip: 100
n_save_model: 20
n_eval: ${n_save_model}
device: "mps"
train_ratio: 0.95
reset_epoch: false
reset_cfg: false
n_train_loss: 1 